{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 565184,
          "sourceType": "datasetVersion",
          "datasetId": 272902
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Predicting Work Absenteeism Rate",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nitindig_absenteeism_at_work_data_set_path = kagglehub.dataset_download('nitindig/absenteeism-at-work-data-set')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Klpnmu2J6sK-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:16.684638Z",
          "iopub.execute_input": "2024-12-22T07:39:16.685103Z",
          "iopub.status.idle": "2024-12-22T07:39:16.719095Z",
          "shell.execute_reply.started": "2024-12-22T07:39:16.685068Z",
          "shell.execute_reply": "2024-12-22T07:39:16.717869Z"
        },
        "id": "18gBCP4P6sLA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries for Data Visualization\n",
        "\n",
        "We will be using the following libraries:\n",
        "- `matplotlib`: A library for creating static, animated, and interactive visualizations.\n",
        "- `seaborn`: A statistical data visualization library built on top of `matplotlib`.\n",
        "- `pandas`: A powerful data manipulation and analysis library.\n",
        "\n",
        "We also set the notebook to display inline plots and apply a `whitegrid` style to `seaborn` for better visual aesthetics.\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n"
      ],
      "metadata": {
        "id": "bMky0siD6sLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:22.16807Z",
          "iopub.execute_input": "2024-12-22T07:39:22.168506Z",
          "iopub.status.idle": "2024-12-22T07:39:22.175446Z",
          "shell.execute_reply.started": "2024-12-22T07:39:22.168461Z",
          "shell.execute_reply": "2024-12-22T07:39:22.174222Z"
        },
        "id": "m8AUaM1m6sLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "We are loading the \"Absenteeism at Work\" dataset using the `pandas` library. The dataset is in CSV format and is being loaded with the following parameters:\n",
        "- `encoding='utf-8'`: Specifies the character encoding used in the file.\n",
        "- `sep=';'`: Defines the separator used in the CSV file (in this case, a semicolon).\n",
        "\n",
        "```python\n",
        "data = pd.read_csv('/kaggle/input/absenteeism-at-work-data-set/Absenteeism_at_work.csv', encoding='utf-8', sep=';')\n"
      ],
      "metadata": {
        "id": "yuLgeV0d6sLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/absenteeism-at-work-data-set/Absenteeism_at_work.csv', encoding='utf-8', sep=';')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:23.825112Z",
          "iopub.execute_input": "2024-12-22T07:39:23.825542Z",
          "iopub.status.idle": "2024-12-22T07:39:23.837074Z",
          "shell.execute_reply.started": "2024-12-22T07:39:23.825509Z",
          "shell.execute_reply": "2024-12-22T07:39:23.83567Z"
        },
        "id": "SopwWUuc6sLC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Numeric Columns\n",
        "\n",
        "We use the `data._get_numeric_data()` function to filter out the numeric columns from the dataset. This function returns a DataFrame that contains only the columns with numeric data types.\n",
        "\n",
        "The `.columns` attribute is then used to display the names of these numeric columns.\n",
        "\n",
        "```python\n",
        "data._get_numeric_data().columns\n"
      ],
      "metadata": {
        "id": "yb377zlq6sLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data._get_numeric_data().columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:26.163207Z",
          "iopub.execute_input": "2024-12-22T07:39:26.163604Z",
          "iopub.status.idle": "2024-12-22T07:39:26.171025Z",
          "shell.execute_reply.started": "2024-12-22T07:39:26.163572Z",
          "shell.execute_reply": "2024-12-22T07:39:26.169618Z"
        },
        "id": "8ZQLmRE46sLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sorting the Data by 'ID' Column\n",
        "\n",
        "We use the `sort_values()` function to sort the data by the `'ID'` column in ascending order. This helps us arrange the data in a way that is easier to analyze or visualize.\n",
        "\n",
        "```python\n",
        "sorted = data.sort_values(by='ID', ascending=True)\n"
      ],
      "metadata": {
        "id": "dZbKv9wA6sLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted=data.sort_values(by='ID', ascending=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:27.369021Z",
          "iopub.execute_input": "2024-12-22T07:39:27.369412Z",
          "iopub.status.idle": "2024-12-22T07:39:27.376285Z",
          "shell.execute_reply.started": "2024-12-22T07:39:27.369382Z",
          "shell.execute_reply": "2024-12-22T07:39:27.374871Z"
        },
        "id": "zmIyAblE6sLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assigning the DataFrame to a New Variable\n",
        "\n",
        "In this step, we assign the original `data` DataFrame (or optionally the `sorted` DataFrame if sorting was done) to the variable `df`. This allows us to continue working with the DataFrame under the new variable name.\n",
        "\n",
        "```python\n",
        "df = data\n"
      ],
      "metadata": {
        "id": "n5JAkwfW6sLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = sorted\n",
        "df = data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:29.605046Z",
          "iopub.execute_input": "2024-12-22T07:39:29.605482Z",
          "iopub.status.idle": "2024-12-22T07:39:29.610258Z",
          "shell.execute_reply.started": "2024-12-22T07:39:29.60545Z",
          "shell.execute_reply": "2024-12-22T07:39:29.608971Z"
        },
        "id": "d8I8bF9I6sLD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a Profiling Report\n",
        "\n",
        "In this step, we use the `ydata_profiling` library to generate a profiling report of the `df` DataFrame. The `ProfileReport` function provides an extensive overview of the dataset, including summaries of each feature, correlations, missing values, and distributions.\n",
        "\n",
        "```python\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "# Create the profiling report\n",
        "profile = ProfileReport(df, title=\"Profiling Report\")\n",
        "\n",
        "# Display the report\n",
        "profile\n"
      ],
      "metadata": {
        "id": "q4QnszJq6sLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "profile = ProfileReport(df, title=\"Profiling Report\")\n",
        "profile"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:30.875945Z",
          "iopub.execute_input": "2024-12-22T07:39:30.876366Z",
          "iopub.status.idle": "2024-12-22T07:39:35.304759Z",
          "shell.execute_reply.started": "2024-12-22T07:39:30.876312Z",
          "shell.execute_reply": "2024-12-22T07:39:35.302223Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "2XBm-Fy16sLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping Columns and Analyzing Correlation\n",
        "\n",
        "In this step, we remove the `ID` and `Seasons` columns from the dataset, as they may not be relevant for the analysis. Afterward, we compute the correlation between the remaining features and the target variable, `Absenteeism time in hours`.\n",
        "\n",
        "```python\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=['ID', 'Seasons'])\n",
        "\n",
        "# Calculate and print the correlation with the target variable\n",
        "print(df.corr()['Absenteeism time in hours'])\n"
      ],
      "metadata": {
        "id": "ptvNd4C06sLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(columns=['ID', 'Seasons'])\n",
        "print(df.corr()['Absenteeism time in hours'])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:39:55.503995Z",
          "iopub.execute_input": "2024-12-22T07:39:55.504381Z",
          "iopub.status.idle": "2024-12-22T07:39:55.513955Z",
          "shell.execute_reply.started": "2024-12-22T07:39:55.504338Z",
          "shell.execute_reply": "2024-12-22T07:39:55.512628Z"
        },
        "id": "YKKVPOXc6sLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Features and Target Variables\n",
        "\n",
        "In this step, we define the target variable `y` and the feature matrix `X`. The target variable is the `Absenteeism time in hours` column, while the feature matrix `X` consists of all other columns in the dataset.\n",
        "\n",
        "```python\n",
        "# Define the target variable 'y' and feature matrix 'X'\n",
        "y = df['Absenteeism time in hours']\n",
        "X = df.drop('Absenteeism time in hours', axis=1)\n",
        "\n",
        "# Display target variable 'y'\n",
        "y\n"
      ],
      "metadata": {
        "id": "rtNCkVIM6sLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=df['Absenteeism time in hours']\n",
        "X=df.drop('Absenteeism time in hours', axis=1)\n",
        "\n",
        "\n",
        "y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:40:03.638208Z",
          "iopub.execute_input": "2024-12-22T07:40:03.638677Z",
          "iopub.status.idle": "2024-12-22T07:40:03.649818Z",
          "shell.execute_reply.started": "2024-12-22T07:40:03.638645Z",
          "shell.execute_reply": "2024-12-22T07:40:03.648521Z"
        },
        "id": "fIa56hQr6sLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the StandardScaler\n",
        "\n",
        "The `StandardScaler` from `sklearn.preprocessing` is used to standardize features by removing the mean and scaling to unit variance. It ensures that each feature in the dataset has a mean of 0 and a standard deviation of 1, which is important for many machine learning algorithms that rely on distance metrics, such as linear regression or k-nearest neighbors.\n",
        "\n",
        "```python\n",
        "# Importing StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "vwieW7Xx6sLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:40:06.93485Z",
          "iopub.execute_input": "2024-12-22T07:40:06.93519Z",
          "iopub.status.idle": "2024-12-22T07:40:06.940024Z",
          "shell.execute_reply.started": "2024-12-22T07:40:06.935165Z",
          "shell.execute_reply": "2024-12-22T07:40:06.93868Z"
        },
        "id": "WUaEqgGB6sLE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the StandardScaler\n",
        "\n",
        "We initialize the `StandardScaler` to standardize the features in the dataset. The `fit()` method will compute the mean and standard deviation for each feature, while `transform()` will standardize the data accordingly.\n",
        "\n",
        "```python\n",
        "# Initializing StandardScaler\n",
        "scaler = StandardScaler()\n"
      ],
      "metadata": {
        "id": "xc4GV6Q66sLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:40:08.214747Z",
          "iopub.execute_input": "2024-12-22T07:40:08.215087Z",
          "iopub.status.idle": "2024-12-22T07:40:08.219867Z",
          "shell.execute_reply.started": "2024-12-22T07:40:08.215054Z",
          "shell.execute_reply": "2024-12-22T07:40:08.218504Z"
        },
        "id": "N3t8R_K16sLF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling the Numeric Features\n",
        "\n",
        "After initializing the `StandardScaler`, we apply it to the numeric features of the dataset. We select the numeric features using `df._get_numeric_data().columns`, and then use `scaler.fit_transform()` to standardize them.\n",
        "\n",
        "```python\n",
        "# Scaling the numeric features\n",
        "num_feat = [feat for feat in df._get_numeric_data().columns]\n",
        "df[num_feat] = scaler.fit_transform(df[num_feat])\n"
      ],
      "metadata": {
        "id": "8rTBRlwf6sLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_feat=[feat for feat in df._get_numeric_data().columns]\n",
        "df[num_feat] = scaler.fit_transform(df[num_feat])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:40:47.835804Z",
          "iopub.execute_input": "2024-12-22T07:40:47.836147Z",
          "iopub.status.idle": "2024-12-22T07:40:47.850055Z",
          "shell.execute_reply.started": "2024-12-22T07:40:47.836122Z",
          "shell.execute_reply": "2024-12-22T07:40:47.848872Z"
        },
        "id": "IzWpZtar6sLF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data into Training and Testing Sets\n",
        "\n",
        "We use the `train_test_split` function from `sklearn.model_selection` to split the data into training and testing sets. 20% of the data is used for testing, and the rest is used for training. A random state of `42` ensures that the split is reproducible.\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "29PNAyN16sLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:40:51.924438Z",
          "iopub.execute_input": "2024-12-22T07:40:51.924798Z",
          "iopub.status.idle": "2024-12-22T07:40:51.935412Z",
          "shell.execute_reply.started": "2024-12-22T07:40:51.92477Z",
          "shell.execute_reply": "2024-12-22T07:40:51.934081Z"
        },
        "id": "99svo3AA6sLF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection with Recursive Feature Elimination (RFE)\n",
        "\n",
        "In this step, we use Recursive Feature Elimination (RFE) to select the top features that contribute the most to the prediction. RFE works by recursively removing features and building a model on the remaining attributes. The process continues until the specified number of features is selected.\n",
        "\n",
        "We use a `LinearRegression` model as the estimator for RFE. Here, we select the top 10 features based on their contribution to predicting the target variable, `Absenteeism time in hours`.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initializing the model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Applying RFE for feature selection\n",
        "selector = RFE(model, n_features_to_select=10)\n",
        "selector.fit(X, y)\n",
        "\n",
        "# Getting the selected features\n",
        "selected_features = X.columns[selector.support_]\n",
        "\n",
        "# Selecting the features for the final model\n",
        "X_selected = X[selected_features]\n",
        "\n",
        "# Fitting the model with the selected features\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Printing the intercept of the model\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "id": "NZ1-sM4E6sLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.feature_selection import RFE\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# model = LinearRegression()\n",
        "# selector = RFE(model, n_features_to_select=10)\n",
        "# selector.fit(X, y)\n",
        "# selected_features = X.columns[selector.support_]\n",
        "\n",
        "# X_selected=X[selected_features]\n",
        "# model.fit(X_train, y_train)\n",
        "# print(model.intercept_)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T22:48:03.775784Z",
          "iopub.execute_input": "2024-12-21T22:48:03.77616Z",
          "iopub.status.idle": "2024-12-21T22:48:03.818477Z",
          "shell.execute_reply.started": "2024-12-21T22:48:03.776131Z",
          "shell.execute_reply": "2024-12-21T22:48:03.815541Z"
        },
        "id": "2BkYFrxN6sLF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polynomial Regression\n",
        "\n",
        "Polynomial regression is a type of regression model that models the relationship between the independent variables (features) and the dependent variable (target) as an nth degree polynomial. This approach can help capture the non-linear relationships between the features and target variable.\n",
        "\n",
        "In this step, we use the `PolynomialFeatures` class from `sklearn` to generate polynomial features of degree 2. These new features will be used to fit a linear regression model, allowing the model to better capture non-linear patterns in the data.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Creating polynomial features of degree 2\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transforming the features into polynomial features\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "\n",
        "# Fitting the linear regression model with polynomial features\n",
        "model.fit(X_poly, y_train)\n",
        "\n",
        "# Making predictions on the test set using the transformed features\n",
        "y_pred = model.predict(poly.transform(X_test))\n"
      ],
      "metadata": {
        "id": "Ii7Ze2qE6sLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "model.fit(X_poly, y_train)\n",
        "y_pred = model.predict(poly.transform(X_test))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T23:28:45.317216Z",
          "iopub.execute_input": "2024-12-21T23:28:45.317638Z",
          "iopub.status.idle": "2024-12-21T23:28:45.353679Z",
          "shell.execute_reply.started": "2024-12-21T23:28:45.317597Z",
          "shell.execute_reply": "2024-12-21T23:28:45.351307Z"
        },
        "id": "E5m3puey6sLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Regression\n",
        "\n",
        "A Decision Tree regressor is a non-linear model that splits the data into different segments based on feature values. It can be used to capture complex relationships between the features and the target variable.\n",
        "\n",
        "In this step, we use the `DecisionTreeRegressor` class from `sklearn` to create a decision tree model. By setting the `max_depth` parameter, we control the depth of the tree, which helps prevent overfitting.\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Initializing the Decision Tree Regressor with max_depth=5 to prevent overfitting\n",
        "tree = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "# Training the model on the training data\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test set\n",
        "y_pred = tree.predict(X_test)\n"
      ],
      "metadata": {
        "id": "yiWr_Wm76sLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor(max_depth=5)\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred = tree.predict(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T23:29:55.78783Z",
          "iopub.execute_input": "2024-12-21T23:29:55.788244Z",
          "iopub.status.idle": "2024-12-21T23:29:55.798681Z",
          "shell.execute_reply.started": "2024-12-21T23:29:55.788211Z",
          "shell.execute_reply": "2024-12-21T23:29:55.797514Z"
        },
        "id": "Q0zO70q26sLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning with Random Forest Regressor\n",
        "\n",
        "In machine learning, hyperparameter tuning is the process of selecting the best hyperparameters for a given model. In this case, we are tuning the hyperparameters of a **Random Forest Regressor** using **GridSearchCV**.\n",
        "\n",
        "1. **RandomForestRegressor**: This is an ensemble learning method that creates multiple decision trees and combines their predictions for a more accurate model.\n",
        "\n",
        "2. **GridSearchCV**: This technique performs an exhaustive search over a specified parameter grid to find the best combination of hyperparameters. It evaluates each combination using cross-validation and chooses the one that minimizes the error (based on the scoring metric).\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Initialize the Random Forest Regressor model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define a hyperparameter grid to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
        "    'max_depth': [10, 20, None],  # Maximum depth of the trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required to be at a leaf node\n",
        "    'max_features\n"
      ],
      "metadata": {
        "id": "Ogbj-iVq6sLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Initialize the model\n",
        "# rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# # Define the hyperparameter grid\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'max_depth': [10, 20, None],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "#     'max_features': ['sqrt', 'log2', None]\n",
        "# }\n",
        "\n",
        "# # Perform grid search with cross-validation\n",
        "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# # Best parameters and model\n",
        "# print(\"Best Parameters:\", grid_search.best_params_)\n",
        "# best_rf = grid_search.best_estimator_\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T23:32:03.684838Z",
          "iopub.execute_input": "2024-12-21T23:32:03.685319Z",
          "iopub.status.idle": "2024-12-21T23:35:32.29345Z",
          "shell.execute_reply.started": "2024-12-21T23:32:03.685281Z",
          "shell.execute_reply": "2024-12-21T23:35:32.291904Z"
        },
        "id": "kW8UkOtU6sLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Regressor Model\n",
        "\n",
        "In this section, we are using the **Random Forest Regressor** to make predictions on the dataset. The Random Forest is an ensemble learning method that uses multiple decision trees to improve the model's predictive accuracy.\n",
        "\n",
        "### Code Explanation:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the Random Forest Regressor with specific hyperparameters\n",
        "forest = RandomForestRegressor(\n",
        "    n_estimators=300,          # Number of trees in the forest (300 trees)\n",
        "    max_depth=10,              # Maximum depth of each tree (limits tree growth to prevent overfitting)\n",
        "    max_features='sqrt',       # The number of features to consider for each split (square root of the total features)\n",
        "    min_samples_split=10,      # The minimum number of samples required to split an internal node\n",
        "    min_samples_leaf=1,        # The minimum number of samples required to be at a leaf node\n",
        "    random_state=42            # Ensures reproducibility by setting the seed for random number generation\n",
        ")\n",
        "\n",
        "# Fit the model to the training data\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "# Use the trained model to predict on the test set\n",
        "y_pred = forest.predict(X_test)\n"
      ],
      "metadata": {
        "id": "n5zg0t8x6sLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest = RandomForestRegressor(n_estimators=300, max_depth=10, max_features='sqrt', min_samples_split=10, min_samples_leaf=1, random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "y_pred = forest.predict(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:41:00.514563Z",
          "iopub.execute_input": "2024-12-22T07:41:00.514923Z",
          "iopub.status.idle": "2024-12-22T07:41:01.077911Z",
          "shell.execute_reply.started": "2024-12-22T07:41:00.514895Z",
          "shell.execute_reply": "2024-12-22T07:41:01.076751Z"
        },
        "id": "YHwVNZ8K6sLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u3g3nok06sLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T23:40:26.785939Z",
          "iopub.execute_input": "2024-12-21T23:40:26.786332Z",
          "iopub.status.idle": "2024-12-21T23:40:26.794377Z",
          "shell.execute_reply.started": "2024-12-21T23:40:26.786303Z",
          "shell.execute_reply": "2024-12-21T23:40:26.793123Z"
        },
        "id": "NeCKlboS6sLG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n",
        "\n",
        "We evaluated the **Random Forest Regressor** model's performance using two key metrics:\n",
        "\n",
        "1. **Mean Squared Error (MSE)**: Measures the average squared difference between the predicted values and actual values. A lower MSE indicates a better fit.\n",
        "\n",
        "2. **R-squared (R²)**: Represents the proportion of variance in the target variable that is predictable from the features. A higher R² value indicates a better model fit.\n",
        "\n",
        "The following code was used for evaluation:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "# Calculate R-squared\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mse  # Output MSE\n",
        "# r2  # Output R-squared\n"
      ],
      "metadata": {
        "id": "GuzAXF4W6sLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse\n",
        "# r2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T07:41:07.624306Z",
          "iopub.execute_input": "2024-12-22T07:41:07.624698Z",
          "iopub.status.idle": "2024-12-22T07:41:07.633029Z",
          "shell.execute_reply.started": "2024-12-22T07:41:07.624671Z",
          "shell.execute_reply": "2024-12-22T07:41:07.631647Z"
        },
        "id": "Ns5JnHXW6sLL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T22:23:08.099622Z",
          "iopub.execute_input": "2024-12-21T22:23:08.099965Z",
          "iopub.status.idle": "2024-12-21T22:23:08.106478Z",
          "shell.execute_reply.started": "2024-12-21T22:23:08.099939Z",
          "shell.execute_reply": "2024-12-21T22:23:08.105134Z"
        },
        "id": "OAODhKmo6sLL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-21T22:23:11.595034Z",
          "iopub.execute_input": "2024-12-21T22:23:11.595544Z",
          "iopub.status.idle": "2024-12-21T22:23:11.602601Z",
          "shell.execute_reply.started": "2024-12-21T22:23:11.595502Z",
          "shell.execute_reply": "2024-12-21T22:23:11.60123Z"
        },
        "id": "TS6r1msj6sLL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Random Forest Model\n",
        "\n",
        "We will save the trained Random Forest model to a file using the `joblib` library. This allows us to reload and use the model later without retraining it.\n",
        "\n",
        "```python\n",
        "import joblib\n",
        "\n",
        "# Save the Random Forest model to a file\n",
        "joblib.dump(forest, 'random_forest_model.pkl')\n"
      ],
      "metadata": {
        "id": "_rha-CFD6sLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(forest, 'random_forest_model.pkl')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T08:38:17.769612Z",
          "iopub.execute_input": "2024-12-22T08:38:17.771625Z",
          "iopub.status.idle": "2024-12-22T08:38:17.926924Z",
          "shell.execute_reply.started": "2024-12-22T08:38:17.771494Z",
          "shell.execute_reply": "2024-12-22T08:38:17.925708Z"
        },
        "id": "a0cUy8wH6sLL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate `requirements.txt`\n",
        "\n",
        "To create a `requirements.txt` file containing all the installed packages in the current environment, use the following command:\n",
        "\n",
        "```python\n",
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "pXb8T9sZ6sLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-22T08:09:07.869667Z",
          "iopub.execute_input": "2024-12-22T08:09:07.870055Z",
          "iopub.status.idle": "2024-12-22T08:09:11.114486Z",
          "shell.execute_reply.started": "2024-12-22T08:09:07.870027Z",
          "shell.execute_reply": "2024-12-22T08:09:11.112867Z"
        },
        "id": "lmcUl-FQ6sLL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CpFB8Kui6sLL"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}